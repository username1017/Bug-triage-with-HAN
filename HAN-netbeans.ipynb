{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import numpy as np\n",
    "import json, re, nltk, string\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    GRU,\n",
    "    Bidirectional,\n",
    "    BatchNormalization,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    RepeatVector,\n",
    "    TimeDistributed,\n",
    "    Permute,\n",
    "    multiply,\n",
    "    Lambda,\n",
    "    Activation,\n",
    ")\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "title_json = './data/netbeans/bugs.json'\n",
    "desc_json = './data/netbeans/longdescs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GLOVE\n",
    "# glove_file = './data/netbeans/vectors.txt'\n",
    "# tmp_file = './data/netbeans/glove.txt'\n",
    "# glove2word2vec(glove_file, tmp_file)\n",
    "# wordvec_model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "# vocabulary = wordvec_model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec parameters\n",
    "min_word_frequency = 5\n",
    "embed_size = 200\n",
    "context_window = 5\n",
    "\n",
    "# NN hyperparameters\n",
    "num_cv = 10\n",
    "max_sentence_num = 20\n",
    "max_sentence_len = 10\n",
    "num_rnn_unit = 512\n",
    "num_dense_unit = 1000\n",
    "rank_k = 10\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "with open(title_json) as data_file:\n",
    "    text = data_file.read()\n",
    "    text = text.replace('\" : NULL', '\" : \"NULL\"')\n",
    "    title_data = json.loads(text, strict=False)\n",
    "    \n",
    "with open(desc_json) as data_file:\n",
    "    text = data_file.read()\n",
    "    text = text.replace('\" : NULL', '\" : \"NULL\"')\n",
    "    desc_data = json.loads(text, strict=False)\n",
    "\n",
    "desc = {}\n",
    "sorted_desc_data = sorted(desc_data, key=lambda x: x['bug_when'], reverse=False)\n",
    "for item in reversed(sorted_desc_data):\n",
    "    temp = {item['bug_id']: item['thetext']}\n",
    "    desc.update(temp) \n",
    "\n",
    "id_title = []\n",
    "for item in title_data:\n",
    "    id_title.append(item['bug_id'])\n",
    "id_desc = list(desc.keys())\n",
    "id_ = list(set(id_title) - set(id_desc))\n",
    "\n",
    "for item in title_data:\n",
    "    if item['bug_id'] in id_:\n",
    "        item['description'] = ''\n",
    "    else:\n",
    "        item['description'] = desc[item['bug_id']]\n",
    "\n",
    "# Netbeans invalid description\n",
    "invalid_desc = ['Priority is changed to P4 (normal).',\n",
    "                ' ']\n",
    "for item in title_data:\n",
    "    if item['description'] in invalid_desc:\n",
    "        title_data.remove(item)\n",
    "\n",
    "open_title = []\n",
    "open_desc = []\n",
    "closed_title = []\n",
    "closed_desc = []\n",
    "closed_owner = []\n",
    "for item in title_data:\n",
    "    # NetBeans\n",
    "    status = ['VERIFIED', 'RESOLVED', 'CLOSED']\n",
    "    if item['bug_status'] in status and item['resolution'] == 'FIXED':\n",
    "        closed_title.append(item['short_desc'])\n",
    "        closed_desc.append(item['description'])\n",
    "        closed_owner.append(item['assigned_to'])\n",
    "    else:\n",
    "        open_title.append(item['short_desc'])\n",
    "        open_desc.append(item['description'])\n",
    "\n",
    "closed_title_20 = []\n",
    "closed_desc_20 = []\n",
    "closed_owner_20 = []\n",
    "owner = {}\n",
    "for key in closed_owner:\n",
    "    owner[key] = owner.get(key, 0) + 1\n",
    "for i in range(len(closed_owner)):\n",
    "    if owner[closed_owner[i]] >= 20:\n",
    "        closed_title_20.append(closed_title[i])\n",
    "        closed_desc_20.append(closed_desc[i])\n",
    "        closed_owner_20.append(closed_owner[i])\n",
    "\n",
    "print(len(open_title))\n",
    "print(len(closed_title))\n",
    "print(len(closed_title_20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Owner details\n",
    "owner_cnt = {}\n",
    "for owner in closed_owner_20:\n",
    "    owner_cnt[owner] = owner_cnt.get(owner, 0) + 1\n",
    "sorted_owner_cnt = sorted(owner_cnt.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in range(10):\n",
    "    print(sorted_owner_cnt[i])\n",
    "print(len(sorted_owner_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def preprocess(title, desc):\n",
    "    # Remove \\r and repeated sentence\n",
    "    current_title = title.replace('\\r', ' ')\n",
    "    current_desc = desc.replace('\\r', ' ')\n",
    "    # Remove URLs\n",
    "    current_desc = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', current_desc)\n",
    "    # Change to lower case\n",
    "    current_title = current_title.lower()\n",
    "    current_desc = current_desc.lower()\n",
    "    # Remove stack trace\n",
    "    start_loc = current_desc.find(\"stack trace\")\n",
    "    current_desc = current_desc[:start_loc]    \n",
    "    # Remove hex code\n",
    "    current_title = re.sub(r'(\\w+)0x\\w+', '', current_title)\n",
    "    current_desc = re.sub(r'(\\w+)0x\\w+', '', current_desc)\n",
    "    # Tokenize sentence\n",
    "    current_title_tokens = nltk.sent_tokenize(current_title)\n",
    "    current_desc_tokens = nltk.sent_tokenize(current_desc)\n",
    "    current_desc_tokens_list = [desc.split('\\n') for desc in current_desc_tokens]\n",
    "    current_desc_tokens = []\n",
    "    for desc in current_desc_tokens_list:\n",
    "        current_desc_tokens += desc\n",
    "    # Remove punctuation\n",
    "    def remove_punct(report):\n",
    "        report_filter = []\n",
    "        for sent in report:\n",
    "            for punct in string.punctuation:\n",
    "                sent = sent.replace(punct, '')\n",
    "            report_filter.append(sent)\n",
    "        return report_filter\n",
    "    current_title_filter = remove_punct(current_title_tokens)\n",
    "    current_desc_filter = remove_punct(current_desc_tokens)\n",
    "    # Tokenize word\n",
    "    current_title_filter = [nltk.word_tokenize(sent) for sent in current_title_filter]\n",
    "    current_desc_filter = [nltk.word_tokenize(sent) for sent in current_desc_filter]\n",
    "    # Lemmatization\n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "    tagged_title = [nltk.pos_tag(title) for title in current_title_filter]\n",
    "    tagged_desc = [nltk.pos_tag(desc) for desc in current_desc_filter]\n",
    "    current_title_lemm = [[WordNetLemmatizer().lemmatize(tag[0], pos=get_wordnet_pos(tag[1]) or wordnet.NOUN) for tag in title] for title in tagged_title]\n",
    "    current_desc_lemm = [[WordNetLemmatizer().lemmatize(tag[0], pos=get_wordnet_pos(tag[1]) or wordnet.NOUN) for tag in desc] for desc in tagged_desc]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    current_title_stop = [[word for word in title if not word in stop_words] for title in current_title_lemm]\n",
    "    current_desc_stop = [[word for word in desc if not word in stop_words] for desc in current_desc_lemm]\n",
    "    # Merge title and description\n",
    "    current_report = current_title_stop + current_desc_stop\n",
    "    current_report = list(filter(None, current_report))\n",
    "    \n",
    "    return current_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bug reports for pre-training word vectors\n",
    "open_report = []\n",
    "open_word = {}\n",
    "for i in range(len(open_title)):\n",
    "    current_report = preprocess(open_title[i], open_desc[i])\n",
    "    # Flatten\n",
    "    current_report = [word for sent in current_report for word in sent]\n",
    "    open_report.append(current_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word vectors\n",
    "wordvec_model = Word2Vec(open_report, min_count=min_word_frequency, size=embed_size, window=context_window)\n",
    "vocabulary = wordvec_model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bug reports for training and testing\n",
    "closed_report = []\n",
    "closed_owner = []\n",
    "for i in range(len(closed_title_20)):\n",
    "    current_report = preprocess(closed_title_20[i], closed_desc_20[i])\n",
    "    closed_report.append(current_report)\n",
    "    closed_owner.append(closed_owner_20[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the words that is not present in the vocabulary\n",
    "update_report = []\n",
    "update_owner = []\n",
    "for i in range(len(closed_owner)):\n",
    "    update_sents = []\n",
    "    for sent in closed_report[i]:\n",
    "        current_sent = [word for word in sent if word in vocabulary]\n",
    "        update_sents.append(current_sent)\n",
    "    update_sents = list(filter(None, update_sents))\n",
    "    update_report.append(update_sents)\n",
    "    update_owner.append(closed_owner[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words to numbers\n",
    "flatten_report = []\n",
    "for report in update_report:\n",
    "    for sent in report:\n",
    "        flatten_report.append(sent)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(flatten_report)\n",
    "\n",
    "for report in update_report:\n",
    "    for sent in report:\n",
    "        for i, word in enumerate(sent):\n",
    "            sent[i] = tokenizer.word_index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = wordvec_model.wv[word]\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define topk_accuracy\n",
    "def topk_accuracy(prediction, y_test, classes, rank_k=10):\n",
    "    accuracy = []\n",
    "    sortedIndices = []\n",
    "    pred_classes = []\n",
    "    for ll in prediction:\n",
    "        sortedIndices.append(\n",
    "            sorted(range(len(ll)), key=lambda ii: ll[ii], reverse=True)\n",
    "        )\n",
    "    for k in range(1, rank_k + 1):\n",
    "        id = 0\n",
    "        trueNum = 0\n",
    "        for sortedInd in sortedIndices:\n",
    "            pred_classes.append(classes[sortedInd[:k]])\n",
    "            if np.argmax(y_test[id]) in sortedInd[:k]:\n",
    "                trueNum += 1\n",
    "            id += 1\n",
    "        accuracy.append((float(trueNum) / len(prediction)) * 100)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def f_measure(prediction, y_test, classes, mode='macro'):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    sortedIndices = []   \n",
    "    for ll in prediction:\n",
    "        sortedIndices.append(\n",
    "            sorted(range(len(ll)), key=lambda ii: ll[ii], reverse=True)\n",
    "        )\n",
    "    id = 0\n",
    "    for sortedInd in sortedIndices:\n",
    "        ind = np.argmax(y_test[id])\n",
    "        if ind in sortedInd[:10]:\n",
    "            y_pred.append(ind)\n",
    "        else:\n",
    "            y_pred.append(-1)\n",
    "        id += 1\n",
    "    for y in y_test:\n",
    "        y_true.append(np.argmax(y))\n",
    "            \n",
    "    f1 = f1_score(y_true, y_pred, average = mode)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# class defining the custom attention layer\n",
    "class HierarchicalAttentionNetwork(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(HierarchicalAttentionNetwork, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim,)))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(HierarchicalAttentionNetwork, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "\n",
    "        ait = K.exp(K.squeeze(K.dot(uit, self.u), -1))\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        weighted_input = x * K.expand_dims(ait)\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "    \n",
    "    def _get_attention_weights(self, X):\n",
    "\n",
    "        uit = K.tanh(K.bias_add(K.dot(X, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        return ait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test\n",
    "splitLength = len(update_report) // (num_cv + 1)\n",
    "slice_results = {}\n",
    "top_rank_k_accuracies = []\n",
    "# f1_measure = []\n",
    "for i in range(1, num_cv + 1):\n",
    "    print(i)\n",
    "    train_report = update_report[:i*splitLength-1]\n",
    "    train_owner = update_owner[:i*splitLength-1]\n",
    "    test_report = update_report[i*splitLength:(i+1)*splitLength-1]\n",
    "    test_owner = update_owner[i*splitLength:(i+1)*splitLength-1]\n",
    "        \n",
    "    # Remove data from test set that is not there in train set\n",
    "    train_owner_unique = set(train_owner)\n",
    "    test_owner_unique = set(test_owner)\n",
    "    unwanted_owner = list(test_owner_unique - train_owner_unique)\n",
    "    update_test_report = []\n",
    "    update_test_owner = []\n",
    "    for i in range(len(test_owner)):\n",
    "        if test_owner[i] not in unwanted_owner:\n",
    "            update_test_report.append(test_report[i])\n",
    "            update_test_owner.append(test_owner[i])\n",
    "    \n",
    "    unique_train_owner = list(set(train_owner))\n",
    "    classes = np.array(unique_train_owner)\n",
    "    \n",
    "    # Create train and test data\n",
    "    X_train = np.zeros(shape=[len(train_report), max_sentence_num, max_sentence_len], dtype=\"int32\")\n",
    "    Y_train = np.zeros(shape=[len(train_owner), 1], dtype=\"int32\")\n",
    "    for i, report in enumerate(train_report):\n",
    "        for j, sent in enumerate(report):\n",
    "            if j < max_sentence_num:\n",
    "                k = 0\n",
    "                for word in sent:\n",
    "                    if k < max_sentence_len:\n",
    "                        X_train[i, j, k] = word\n",
    "                        k = k + 1\n",
    "        Y_train[i, 0] = unique_train_owner.index(train_owner[i])\n",
    "    \n",
    "    X_test = np.zeros(shape=[len(update_test_report), max_sentence_num, max_sentence_len], dtype=\"int32\")\n",
    "    Y_test = np.zeros(shape=[len(update_test_owner), 1], dtype=\"int32\")\n",
    "    for i, report in enumerate(update_test_report):\n",
    "        for j, sent in enumerate(report):\n",
    "            if j < max_sentence_num:\n",
    "                k = 0\n",
    "                for word in sent:\n",
    "                    if k < max_sentence_len:\n",
    "                        X_test[i, j, k] = word\n",
    "                        k = k + 1\n",
    "        Y_test[i, 0] = unique_train_owner.index(update_test_owner[i])    \n",
    "    \n",
    "    y_train = np_utils.to_categorical(Y_train, len(unique_train_owner))\n",
    "    y_test = np_utils.to_categorical(Y_test, len(unique_train_owner))\n",
    "    \n",
    "    # Model\n",
    "    word_input = Input(shape=(max_sentence_len,), dtype='float32')\n",
    "    embedded_sequences = Embedding(len(embedding_matrix), embed_size, weights=[embedding_matrix], input_length=max_sentence_len, trainable=True)(word_input)\n",
    "    l_gru = Bidirectional(GRU(num_rnn_unit, return_sequences=True, dropout=0.2))(embedded_sequences)\n",
    "    l_dense = TimeDistributed(Dense(num_dense_unit))(l_gru)\n",
    "    l_att = HierarchicalAttentionNetwork(max_sentence_num)(l_dense)\n",
    "    word_encoder = Model(word_input, l_att)\n",
    "    \n",
    "    sent_input = Input(shape=(max_sentence_num, max_sentence_len), dtype='float32')\n",
    "    sent_encoder = TimeDistributed(word_encoder)(sent_input)\n",
    "    l_gru_sent = Bidirectional(GRU(num_rnn_unit, return_sequences=True, dropout=0.2))(sent_encoder)\n",
    "    l_dense_sent = TimeDistributed(Dense(num_dense_unit))(l_gru_sent)\n",
    "    l_att_sent = HierarchicalAttentionNetwork(max_sentence_len)(l_dense_sent)\n",
    "    preds = Dense(len(classes), activation='softmax')(l_att_sent)\n",
    "    model = Model(sent_input, preds)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=Adam(lr=1e-4), metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3)\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=500, callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "    prediction = model.predict(X_test)\n",
    "    accuracy = topk_accuracy(prediction, y_test, classes, rank_k=rank_k)\n",
    "#     f1 = f_measure(prediction, y_test, classes, mode='macro')\n",
    "    print(\"CV{0}, top1 - ... - top{1} accuracy: \".format(i, rank_k), accuracy)\n",
    "    \n",
    "    train_result = hist.history\n",
    "    train_result[\"test_topk_accuracies\"] = accuracy\n",
    "    slice_results[i + 1] = train_result\n",
    "    top_rank_k_accuracies.append(accuracy[-1])\n",
    "#     f1_measure.append(f1)\n",
    "    \n",
    "    del model\n",
    "    \n",
    "print(\"Top{0} accuracies for all CVs: {1}\".format(rank_k, top_rank_k_accuracies))\n",
    "print(\"Average top{0} accuracy: {1}\".format(rank_k, sum(top_rank_k_accuracies)/rank_k))\n",
    "# print(f1_measure)\n",
    "# print(np.mean(f1_measure))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
